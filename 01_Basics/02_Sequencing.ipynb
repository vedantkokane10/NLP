{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "e-Mg1qSfFWhj"
      },
      "outputs": [],
      "source": [
        "# Sequencing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        ""
      ],
      "metadata": {
        "id": "Cs7_uSsEOPLC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"I love my dog\",\n",
        "             \"I love my cat\",\n",
        "             \"Do you love my Dog?\"]"
      ],
      "metadata": {
        "id": "FZHSn6jKOQPt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = 100)  # num_words = 100 -> 100 most frequent words\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index       # mapping of words with numeric values\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6hrVnL2OSe_",
        "outputId": "3e280f56-6091-440a-b892-13f42b30bc99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'do': 6, 'you': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)  # it will generate the sequence of words i.e tokens (numeric values) in array format"
      ],
      "metadata": {
        "id": "XmqJGHudOURP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zGUxvT9Oc5A",
        "outputId": "86fd916d-3946-489e-d443-781d4b1fa7d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 1, 2, 4], [3, 1, 2, 5], [6, 7, 1, 2, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# padding sequences\n",
        "# While processing the text we create arrays of tokens of the sentences\n",
        "# in order to have arrays of unifrom length we can use padding\n",
        "# it will set 0 as empty token\n"
      ],
      "metadata": {
        "id": "Rmz91kgFOwZW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_sequences = pad_sequences(sequences, padding = 'pre') # can set to post  as well\n",
        "print(padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-7pG_y3YagN",
        "outputId": "95034605-aaea-4e6d-9f72-cb038a1901a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 3 1 2 4]\n",
            " [0 3 1 2 5]\n",
            " [6 7 1 2 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQvhTV3WYnnO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}